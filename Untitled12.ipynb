{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "from dask.distributed import Client\n",
        "import time\n",
        "\n",
        "# Define the file paths to the datasets\n",
        "trips_by_distance_path = '/content/Trips_by_Distance.csv'\n",
        "trips_full_data_path = '/content/Trips_Full Data.csv'\n",
        "\n",
        "# Define a function for serial computation using pandas\n",
        "def compute_serial(file_path):\n",
        "    # Read the data using pandas\n",
        "    df = pd.read_csv(file_path)\n",
        "    # Calculate the total distance by summing the relevant columns\n",
        "    # Adjust the column names according to your dataset\n",
        "    df['Total Distance'] = df['Trips <1 Mile'] + df['Trips 1-3 Miles'] + df['Trips 3-5 Miles'] + df['Trips 5-10 Miles']\n",
        "    # Calculate the average distance\n",
        "    return df['Total Distance'].mean()\n",
        "\n",
        "# Define a function for parallel computation using Dask\n",
        "def compute_parallel(file_path, n_workers):\n",
        "    # Setup Dask client with specified number of workers\n",
        "    client = Client(n_workers=n_workers)\n",
        "    # Read the data using Dask\n",
        "    df = dd.read_csv(file_path)\n",
        "    # Perform the same calculation as above, in parallel\n",
        "    df['Total Distance'] = df['Trips <1 Mile'] + df['Trips 1-3 Miles'] + df['Trips 3-5 Miles'] + df['Trips 5-10 Miles']\n",
        "    avg_distance = df['Total Distance'].mean().compute()\n",
        "    # Close the Dask client\n",
        "    client.close()\n",
        "    return avg_distance\n",
        "\n",
        "# Perform serial computation and measure the time taken\n",
        "start_time = time.time()\n",
        "avg_distance_serial = compute_serial(trips_full_data_path)\n",
        "end_time = time.time()\n",
        "print(f\"Average distance traveled (serial): {avg_distance_serial}\")\n",
        "print(f\"Time taken (serial): {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "# Perform parallel computations with different number of processors and measure the time taken\n",
        "processor_counts = [10, 20]\n",
        "times_taken = {}\n",
        "\n",
        "for count in processor_counts:\n",
        "    start_time = time.time()\n",
        "    avg_distance_parallel = compute_parallel(trips_full_data_path, count)\n",
        "    end_time = time.time()\n",
        "    times_taken[count] = end_time - start_time\n",
        "    print(f\"Time taken with {count} processors: {times_taken[count]:.4f} seconds\")\n",
        "\n",
        "# Output the results\n",
        "print(\"\\nSerial processing time:\", end_time - start_time)\n",
        "for count, time_taken in times_taken.items():\n",
        "    print(f\"Parallel processing time with {count} processors: {time_taken:.4f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8d5Fb-ISoG5",
        "outputId": "5f1c2608-2936-4765-e477-ce6916018080"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average distance traveled (serial): 1110453316.857143\n",
            "Time taken (serial): 0.0090 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.http.proxy:To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
            "INFO:distributed.scheduler:State start\n",
            "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:35915\n",
            "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:8787/status\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:43145'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:34443'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:35533'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:46787'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:43225'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:33275'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:43011'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:37797'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:42873'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:36589'\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:36373', name: 2, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:36373\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:49270\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:35821', name: 8, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:35821\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:49264\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:36319', name: 9, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:36319\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:49290\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:43193', name: 0, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:43193\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:49286\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:35301', name: 3, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:35301\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:49294\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:35727', name: 7, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:35727\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:49304\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:41607', name: 4, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:41607\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:49288\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:40773', name: 1, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:40773\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:49302\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:38867', name: 5, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:38867\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:49298\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:45571', name: 6, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:45571\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:49314\n",
            "INFO:distributed.scheduler:Receive client connection: Client-683f5fc6-f7b7-11ee-80dd-0242ac1c000c\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:58840\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:43145'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:34443'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:35533'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:46787'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:43225'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:33275'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:43011'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:37797'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:42873'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:36589'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:49294; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:49286; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:49302; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:49270; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:49288; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:49298; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:49314; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:49304; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:49264; closing.\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:35301', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807777.8977857')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:43193', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807777.9017713')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:40773', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807777.9254818')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:36373', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807777.9398327')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:41607', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807777.9699984')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:38867', name: 5, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807777.9764605')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:45571', name: 6, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807777.999255')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:35727', name: 7, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807778.0022914')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:35821', name: 8, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807778.0366914')\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:49290; closing.\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:36319', name: 9, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807778.2007527')\n",
            "INFO:distributed.scheduler:Lost all workers\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.199998474121094 seconds, killing\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999986267089846 seconds, killing\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999990844726565 seconds, killing\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999983215332035 seconds, killing\n",
            "INFO:distributed.nanny:Worker process 1658 was killed by signal 9\n",
            "INFO:distributed.nanny:Worker process 1661 was killed by signal 9\n",
            "INFO:distributed.nanny:Worker process 1663 was killed by signal 9\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.199998474121094 seconds, killing\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999987792968754 seconds, killing\n",
            "INFO:distributed.nanny:Worker process 1667 was killed by signal 9\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999986267089846 seconds, killing\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999989318847657 seconds, killing\n",
            "INFO:distributed.nanny:Worker process 1673 was killed by signal 9\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999987792968754 seconds, killing\n",
            "INFO:distributed.nanny:Worker process 1670 was killed by signal 9\n",
            "INFO:distributed.nanny:Worker process 1676 was killed by signal 9\n",
            "INFO:distributed.nanny:Worker process 1682 was killed by signal 9\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999983215332035 seconds, killing\n",
            "INFO:distributed.nanny:Worker process 1679 was killed by signal 9\n",
            "INFO:distributed.nanny:Worker process 1684 was killed by signal 9\n",
            "INFO:distributed.scheduler:Scheduler closing due to unknown reason...\n",
            "INFO:distributed.scheduler:Scheduler closing all comms\n",
            "INFO:distributed.scheduler:State start\n",
            "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:39903\n",
            "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:8787/status\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken with 10 processors: 21.1884 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:43931'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:43979'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:34343'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:36669'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:34433'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:41675'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:37075'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:44339'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:42707'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:35795'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:35849'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:42455'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:37491'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:41015'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:38009'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:36755'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:35539'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:37775'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:33893'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:46221'\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:46681', name: 10, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:46681\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:48348\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:35807', name: 9, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:35807\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:48382\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:41365', name: 3, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:41365\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:48406\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:33201', name: 6, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:33201\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:48362\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:35403', name: 15, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:35403\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:48396\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:32859', name: 19, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:32859\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:48384\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:42375', name: 0, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:42375\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:48372\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:34097', name: 11, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:34097\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:48430\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:44585', name: 5, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:44585\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:48418\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:32787', name: 7, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:32787\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:48462\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:34043', name: 18, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:34043\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:48458\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:40275', name: 1, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:40275\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:48416\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:35983', name: 17, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:35983\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:48504\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:37233', name: 4, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:37233\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:48492\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:46377', name: 13, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:46377\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:48476\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:46257', name: 8, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:46257\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:48502\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:40719', name: 14, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:40719\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:48444\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:42947', name: 2, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:42947\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:48494\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:41485', name: 12, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:41485\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:48496\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:42611', name: 16, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:42611\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:48508\n",
            "INFO:distributed.scheduler:Receive client connection: Client-74e07a3e-f7b7-11ee-80dd-0242ac1c000c\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:43588\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:43931'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:43979'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:34343'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:36669'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:34433'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:41675'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:37075'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:44339'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:42707'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:35795'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:35849'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:42455'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:37491'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:41015'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:38009'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:36755'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:35539'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:37775'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:33893'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:46221'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:48372; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:48416; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:48494; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:48406; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:48492; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:48418; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:48362; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:48502; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:48382; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:48348; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:48430; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:48496; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:48476; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:48444; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:48396; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:48508; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:48504; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:48458; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:48462; closing.\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:42375', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807817.2427075')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:40275', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807817.2858326')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:42947', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807817.2884622')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:41365', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807817.3368778')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:37233', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807817.3433218')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:44585', name: 5, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807817.3922646')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:33201', name: 6, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807817.3947024')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:46257', name: 8, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807817.424373')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:35807', name: 9, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807817.4580178')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:46681', name: 10, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807817.4737175')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:34097', name: 11, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807817.4770312')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:41485', name: 12, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807817.5168948')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:46377', name: 13, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807817.5239065')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:40719', name: 14, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807817.5736735')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:35403', name: 15, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807817.5887563')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:42611', name: 16, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807817.6170793')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:35983', name: 17, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807817.646114')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:34043', name: 18, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807817.6612816')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:32787', name: 7, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807817.7402062')\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:48384; closing.\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:32859', name: 19, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1712807818.1448236')\n",
            "INFO:distributed.scheduler:Lost all workers\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999986267089846 seconds, killing\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999987792968754 seconds, killing\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999980163574224 seconds, killing\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999987792968754 seconds, killing\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999989318847657 seconds, killing\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999986267089846 seconds, killing\n",
            "INFO:distributed.nanny:Worker process 1862 was killed by signal 9\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999990844726565 seconds, killing\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999990844726565 seconds, killing\n",
            "INFO:distributed.nanny:Worker process 1871 was killed by signal 9\n",
            "INFO:distributed.nanny:Worker process 1865 was killed by signal 9\n",
            "INFO:distributed.nanny:Worker process 1868 was killed by signal 9\n",
            "INFO:distributed.nanny:Worker process 1874 was killed by signal 9\n",
            "INFO:distributed.nanny:Worker process 1877 was killed by signal 9\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999986267089846 seconds, killing\n",
            "INFO:distributed.nanny:Worker process 1882 was killed by signal 9\n",
            "INFO:distributed.nanny:Worker process 1881 was killed by signal 9\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999989318847657 seconds, killing\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999983215332035 seconds, killing\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999989318847657 seconds, killing\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999986267089846 seconds, killing\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999989318847657 seconds, killing\n",
            "INFO:distributed.nanny:Worker process 1886 was killed by signal 9\n",
            "INFO:distributed.nanny:Worker process 1889 was killed by signal 9\n",
            "INFO:distributed.nanny:Worker process 1895 was killed by signal 9\n",
            "INFO:distributed.nanny:Worker process 1892 was killed by signal 9\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999989318847657 seconds, killing\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999981689453127 seconds, killing\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999978637695317 seconds, killing\n",
            "INFO:distributed.nanny:Worker process 1902 was killed by signal 9\n",
            "INFO:distributed.nanny:Worker process 1905 was killed by signal 9\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999980163574224 seconds, killing\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999981689453127 seconds, killing\n",
            "WARNING:distributed.nanny:Worker process still alive after 3.1999983215332035 seconds, killing\n",
            "INFO:distributed.nanny:Worker process 1909 was killed by signal 9\n",
            "INFO:distributed.nanny:Worker process 1911 was killed by signal 9\n",
            "INFO:distributed.nanny:Worker process 1914 was killed by signal 9\n",
            "INFO:distributed.nanny:Worker process 1917 was killed by signal 9\n",
            "INFO:distributed.nanny:Worker process 1920 was killed by signal 9\n",
            "INFO:distributed.nanny:Worker process 1922 was killed by signal 9\n",
            "INFO:distributed.scheduler:Scheduler closing due to unknown reason...\n",
            "INFO:distributed.scheduler:Scheduler closing all comms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken with 20 processors: 39.0907 seconds\n",
            "\n",
            "Serial processing time: 39.09074783325195\n",
            "Parallel processing time with 10 processors: 21.1884 seconds\n",
            "Parallel processing time with 20 processors: 39.0907 seconds\n"
          ]
        }
      ]
    }
  ]
}